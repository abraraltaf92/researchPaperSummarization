Comparable corpora such as news documents of the same period from different news agencies are readily available.
In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.
Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).
But parallel corpora are scarce resources, especially for uncommon lan guage pairs.
Comparable corpora refer to texts that are not direct translation but are about the same topic.
Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).
When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.
On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.
The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.
Each of the two individual methods provides a P(Q | D) is the one that best matches the query.
If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general.
∏ t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.
Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.
We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.
For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates.
Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).
During the whole December period, we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words.
We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.
‘insuff’ means the correct translation appears less than 10 times in the English part of the comparable corpus.
On the other hand, using our method of combining both sources of information and setting M = ∞, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except 巴佐亚,坩埚,普利法) have their correct English translations at rank one position.
On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.
In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone.
The work that is most similar to ours is the recent research of (Huang et al., 2004).
They attempted to improve named entity translation by combining phonetic and semantic information.
They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.
In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.Comparable corpora such as news documents of the same period from different news agencies are readily available.
In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.
Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).
But parallel corpora are scarce resources, especially for uncommon lan guage pairs.
Comparable corpora refer to texts that are not direct translation but are about the same topic.
Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).
When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.
On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.
The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.
Each of the two individual methods provides a P(Q | D) is the one that best matches the query.
If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general.
∏ t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.
Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.
We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.
For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates.
Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).
During the whole December period, we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words.
We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.
‘insuff’ means the correct translation appears less than 10 times in the English part of the comparable corpus.
On the other hand, using our method of combining both sources of information and setting M = ∞, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except 巴佐亚,坩埚,普利法) have their correct English translations at rank one position.
On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.
In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone.
The work that is most similar to ours is the recent research of (Huang et al., 2004).
They attempted to improve named entity translation by combining phonetic and semantic information.
They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.
In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.