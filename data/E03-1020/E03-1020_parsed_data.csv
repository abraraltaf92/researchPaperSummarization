paper_title,Discovering Corpus-Specific Word Senses
abstract,This paper presents an unsupervised algorithm which automatically discovers word senses from text.
abstract,The algorithm is based on a graph model representing words and relationships between them.
abstract,Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.
abstract,Discrimination against previously extracted sense clusters enables us to discover new senses.
abstract,We use the same data for both recognising and resolving ambiguity.
Introduction,This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.
Introduction,Automatic word sense discovery has applications of many kinds.
Introduction,It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones.
Introduction,"The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998)."
Introduction,This paper is organised as follows.
Introduction,"In section 2, we present the graph model from which we discover word senses."
Introduction,"Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000)."
Introduction,The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.
Introduction,"In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning."
Introduction,We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.
Introduction,Section 5 describes the experiment and presents a sample of the results.
Introduction,"Finally, section 6 sketches applications of the algorithm and discusses future work."
Building a Graph of Similar Words.,"The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech."
Building a Graph of Similar Words.,"Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. ""genomic DNA from rat, mouse and dog""."
Building a Graph of Similar Words.,"Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1."
Building a Graph of Similar Words.,"Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words."
Building a Graph of Similar Words.,The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.
Building a Graph of Similar Words.,1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words.
Building a Graph of Similar Words.,Instead we link each word to its top n neighbors where n can be determined by the user (cf.
Building a Graph of Similar Words.,section 4)..
Building a Graph of Similar Words.,41=0 441=P .4161.
Building a Graph of Similar Words.,"sz44, CD miltrA, litrepate inovio.� h,) Cik Figure 1: Local graph of the word mouse"
Markov Clustering.,"Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse."
Markov Clustering.,"However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense."
Markov Clustering.,"There are, of course, many more types of polysemy (cf."
Markov Clustering.,e.g.
Markov Clustering.,"(Kilgarriff, 1992))."
Markov Clustering.,"As can be seen in figure 2, wing ""part of a bird"" is closely related to tail, as is wing ""part of a plane""."
Markov Clustering.,"Therefore, even after removal of the wing-node, the two areas of meaning are still linked via tail."
Markov Clustering.,"The same happens with wing ""part of a building"" and wing ""political group"" which are linked via policy."
Markov Clustering.,"However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning."
Markov Clustering.,"To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000)."
Markov Clustering.,The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters.
Markov Clustering.,The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).
Markov Clustering.,"Let G�, denote the local graph around the ambiguous word w. The adjacency matrix MG� 4111) 11� 41 4Wit ler,1110.1/.17, cgtoserek�Ilt Figure 2: Local graph of the word wing of a graph G�, is defined by setting (111G�) pq equal to the weight of the edge between nodes v and v q . Normalizing the columns of A/G� results in the Markov Matrix Taw whose entries (Thi,)pq can be interpreted as transition probability from v q to vv . It can easily be shown that the k-th power of TG� lists the probabilities (TL )pq of a path of length k starting at node vq and ending at node V. The MCL-algorithm simulates flow in Gw by iteratively recomputing the set of transition probabilities via two steps, expansion and inflation."
Markov Clustering.,The expansion step corresponds with taking the k-th power of TG� as outlined above and allows nodes to see new neighbours.
Markov Clustering.,"The inflation step takes each matrix entry to the r-th power and then rescales each column so that the entries sum to 1.Vi a inflation, popular neighbours are further supported at the expense of less popular ones."
Markov Clustering.,Flow within dense regions in the graph is concentrated by both expansion and inflation.
Markov Clustering.,"Eventually, flow between dense regions will disappear, the matrix of transition probabilities TG� will converge and the limiting matrix can be interpreted as a clustering of the graph."
Word Sense Clustering Algorithm.,The output of the MCL-algorithm strongly depends on the inflation and expansion parameters r and k as well as the size of the local graph which serves as input to MCL.
Word Sense Clustering Algorithm.,An appropriate choice of the inflation param 80 eter r can depend on the ambiguous word w to be clustered.
Word Sense Clustering Algorithm.,"In case of homonymy, a small inflation parameter r would be appropriate."
Word Sense Clustering Algorithm.,"However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another."
Word Sense Clustering Algorithm.,"In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together."
Word Sense Clustering Algorithm.,"Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus."
Word Sense Clustering Algorithm.,"If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus."
Word Sense Clustering Algorithm.,"On the other hand, if the local graph is too big, we will get a lot of noise."
Word Sense Clustering Algorithm.,"Below, we outline an algorithm which circumvents the problem of choosing the right parameters."
Word Sense Clustering Algorithm.,"In contrast to pure Markov clustering, we don't try to find a complete clustering of G into senses at once."
Word Sense Clustering Algorithm.,"Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only."
Word Sense Clustering Algorithm.,We then recompute the local graph Gw by discriminating against c's features.
Word Sense Clustering Algorithm.,"This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words."
Word Sense Clustering Algorithm.,The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold.
Word Sense Clustering Algorithm.,"Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty."
Word Sense Clustering Algorithm.,The algorithm consists of the following steps: 1.
Word Sense Clustering Algorithm.,Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.
Word Sense Clustering Algorithm.,2. Recursively remove all nodes of degree one.
Word Sense Clustering Algorithm.,Then remove the node corresponding with w from G. 3.
Word Sense Clustering Algorithm.,Apply MCL to Gw with a fairly big inflation parameter r which is fixed.
Word Sense Clustering Algorithm.,4.
Word Sense Clustering Algorithm.,"Take the ""best"" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5."
Word Sense Clustering Algorithm.,Go back to 1 with the reduced/devalued set of features F. 6.
Word Sense Clustering Algorithm.,Go through the final list of clusters L and assign a name to each cluster using a broad-coverage taxonomy (see below).
Word Sense Clustering Algorithm.,"Merge semantically close clusters using a taxonomy-based semantic distance measure (Budanitsky and Hirst, 2001) and assign a class-label to the newly formed cluster."
Word Sense Clustering Algorithm.,7.
Word Sense Clustering Algorithm.,Output the list of class-labels which best represent the different senses of w in the corpus.
Word Sense Clustering Algorithm.,"The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the ""best"" cluster, it suffices to build a relatively small graph in 1."
Word Sense Clustering Algorithm.,"Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them."
Word Sense Clustering Algorithm.,"In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted."
Word Sense Clustering Algorithm.,"The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose."
Word Sense Clustering Algorithm.,The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label.
Word Sense Clustering Algorithm.,"The family of such algorithms is described in (Widdows, 2003)."
Experimental Results.,"In this section, we describe an initial evaluation experiment and present the results."
Experimental Results.,We will soon carry out and report on a more thorough analysis of our algorithm.
Experimental Results.,We used the simple graph model based on co-occurrences of nouns in lists (cf.
Experimental Results.,section 2) for our experiment.
Experimental Results.,"We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry)."
Experimental Results.,"Our algorithm was applied to each word in the list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) in order to extract the top two sense clusters only."
Experimental Results.,We then determined the WordNet synsets which most adequately characterized the sense clusters.
Experimental Results.,An extract of the results is listed in table 1.
Experimental Results.,"Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output of word sense clustering."
Applications and future research.,"The benefits of automatic, data-driven word sense discovery for natural language processing and lexicography would be very great."
Applications and future research.,Here we only mention a few direct results of our work.
Applications and future research.,"Our algorithm does not only recognise ambiguity, but can also be used to resolve it, because the features shared by the members of each sense cluster provide strong indication of which reading of an ambiguous word is appropriate given a certain context."
Applications and future research.,"This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated."
Applications and future research.,The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning.
Applications and future research.,This approach to disambiguation combines the benefits of both Yarowsky's (1995) and Schtitze's (1998) approaches.
Applications and future research.,Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used.
Applications and future research.,Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted.
Applications and future research.,"They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora."
Applications and future research.,The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus.
Applications and future research.,We prepare an evaluation of our algorithm as applied to the collocation relationships (cf.
Applications and future research.,"section 2), and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly."
